import time
import logging
import json
from datetime import datetime, timedelta
from app.services.feishu_service import FeishuService
from app.core.llm import LLMClient
from app.core.prompts import PROMPTS, PromptTemplate
from app.core.config import settings

logger = logging.getLogger(__name__)

class ReportAnalysisService:
    def __init__(self):
        self.llm_client = LLMClient()

    async def sync_and_analyze(self, hours: int = 24):
        """
        åŒæ­¥å¹¶åˆ†æé£ä¹¦æ±‡æŠ¥
        :param hours: æŸ¥è¯¢è¿‡å»å¤šå°‘å°æ—¶çš„æ±‡æŠ¥
        """
        if not settings.FEISHU_BITABLE_APP_TOKEN or not settings.FEISHU_BITABLE_TABLE_ID:
            logger.error("Missing Bitable configuration (FEISHU_BITABLE_APP_TOKEN or FEISHU_BITABLE_TABLE_ID)")
            print("âŒ é…ç½®ç¼ºå¤±: è¯·åœ¨ .env ä¸­è®¾ç½® FEISHU_BITABLE_APP_TOKEN å’Œ FEISHU_BITABLE_TABLE_ID")
            return

        # --- A. è®¾å®šæŸ¥è¯¢æ—¶é—´èŒƒå›´ ---
        now = int(time.time())
        start_time = now - (hours * 3600)
        
        print(f"ğŸ“¥ æ­£åœ¨ä»é£ä¹¦æ±‡æŠ¥åº”ç”¨æ‹‰å–æ•°æ® (è¿‡å» {hours} å°æ—¶)...")
        
        # --- B. Extract (æå–) ---
        tasks = await FeishuService.get_report_tasks(start_time, now)
        
        if not tasks:
            print(f"ğŸ“­ è¿‡å» {hours} å°æ—¶å†…æ²¡æœ‰æ–°çš„æ±‡æŠ¥ã€‚")
            return

        print(f"âœ… è·å–åˆ° {len(tasks)} æ¡æ±‡æŠ¥ï¼Œå¼€å§‹å¤„ç†...")
        
        # --- C. Delete-Then-Insert Preparation (åˆ é™¤åå†™å…¥æ¨¡å¼) ---
        # 1. æŸ¥è¯¢å¤šç»´è¡¨æ ¼ä¸­å·²å­˜åœ¨çš„è®°å½•ï¼Œæ„å»ºæ˜ å°„è¡¨ï¼Œç”¨äºåˆ é™¤æ—§è®°å½•
        # è¿‡æ»¤æ¡ä»¶: æ±‡æŠ¥æ—¥æœŸ >= start_time * 1000
        # æ˜ å°„è¡¨: (user_id, date_str_yyyy_mm_dd) -> [record_ids]
        existing_map_by_id = {}
        existing_map_by_name = {}
        
        try:
            # æ„é€ ç­›é€‰æ¡ä»¶ï¼šåªæŸ¥è¯¢å¼€å§‹æ—¶é—´ä¹‹åçš„è®°å½• (å‘å‰å®½é™1å¤©ä»¥å¤„ç†æ—¶åŒºå·®å¼‚)
            search_start_time = (start_time - 86400) * 1000
            # æ³¨æ„ï¼šæ—¥æœŸç­›é€‰å¯èƒ½éœ€è¦æ ¹æ® Bitable å­—æ®µè®¾ç½®è°ƒæ•´ï¼Œæ­¤å¤„å‡è®¾ä¸ºæ¯«ç§’æ—¶é—´æˆ³
            # ç”±äº Bitable ç´¢å¼•å»¶è¿Ÿå¯¼è‡´åˆšå†™å…¥çš„è®°å½•å¯èƒ½æ— æ³•ç«‹å³é€šè¿‡ Filter æŸ¥åˆ°ï¼Œä¸ºç¡®ä¿å»é‡é€»è¾‘å¯é ï¼Œæš‚æ—¶ç§»é™¤ Filter
            # filter_str = f'CurrentValue.[æ±‡æŠ¥æ—¥æœŸ] >= {search_start_time}'
            
            existing_records = await FeishuService.search_bitable_records(
                settings.FEISHU_BITABLE_APP_TOKEN,
                settings.FEISHU_BITABLE_TABLE_ID,
                # filter_str=filter_str
            )
            logger.info(f"Existing records search result count: {len(existing_records) if existing_records else 0}")
            
            if existing_records:
                for record in existing_records:
                    record_id = getattr(record, 'record_id', '')
                    fields = getattr(record, 'fields', {})
                    # logger.info(f"Checking record {record_id} for deduplication. Fields: {fields}")
                    
                    # è·å–æ±‡æŠ¥æ—¥æœŸ
                    report_date = fields.get('æ±‡æŠ¥æ—¥æœŸ')
                    if not report_date:
                        # logger.warning(f"Record {record_id} missing 'æ±‡æŠ¥æ—¥æœŸ', skipping.")
                        continue
                        
                    # è½¬æ¢ä¸º YYYY-MM-DD å­—ç¬¦ä¸² (Bitable å­˜å‚¨çš„æ˜¯æ¯«ç§’)
                    dt = datetime.fromtimestamp(int(report_date) / 1000)
                    date_str = dt.strftime('%Y-%m-%d')
                    
                    # 1. æŒ‰æ±‡æŠ¥äºº ID åŒ¹é… (Person field)
                    reporters = fields.get('æ±‡æŠ¥äºº', [])
                    reporter_id = None
                    if reporters and isinstance(reporters, list):
                        reporter_id = reporters[0].get('id')
                    
                    if reporter_id:
                        key = (reporter_id, date_str)
                        if key not in existing_map_by_id:
                            existing_map_by_id[key] = []
                        existing_map_by_id[key].append(record_id)
                        # logger.info(f"Mapped record {record_id} to ID key {key}")
                    
                    # 2. æŒ‰æäº¤äººå§“ååŒ¹é… (Fallback for Text/Person field)
                    submitter = fields.get('æäº¤äºº')
                    submitter_name = None
                    if isinstance(submitter, list) and len(submitter) > 0:
                         submitter_name = submitter[0].get('name')
                    elif isinstance(submitter, dict):
                         submitter_name = submitter.get('name')
                    elif isinstance(submitter, str):
                         submitter_name = submitter
                         
                    if submitter_name:
                        key = (submitter_name, date_str)
                        if key not in existing_map_by_name:
                            existing_map_by_name[key] = []
                        existing_map_by_name[key].append(record_id)
                        # logger.info(f"Mapped record {record_id} to Name key {key}")
                        
                print(f"ğŸ” å‘ç° {len(existing_records)} æ¡å·²æœ‰è®°å½•ï¼Œå°†è¿›è¡Œå»é‡å¤„ç†ã€‚")
                
        except Exception as e:
            logger.warning(f"Failed to fetch existing records for deduplication: {e}")
        
        # é¢„å…ˆæ‰¹é‡è·å–ç”¨æˆ·ä¿¡æ¯
        user_ids = list(set([task.from_user_id for task in tasks if hasattr(task, 'from_user_id')]))
        user_map = {}
        if user_ids:
            users = await FeishuService.batch_get_users(user_ids)
            if users:
                for user in users:
                    user_map[user.user_id] = user.name

        # --- D. Filter Tasks (æœ¬åœ°è¿‡æ»¤: æ¯å¤©æ¯äººåªä¿ç•™æœ€æ–°ä¸€æ¡) ---
        # Map: (user_id, date_str) -> task
        filtered_tasks_map = {}
        
        for task in tasks:
            user_id = getattr(task, 'from_user_id', '')
            # å¦‚æœæ²¡æœ‰ user_idï¼Œå°è¯•ç”¨åå­—ä½œä¸º key (ä¸å¤ªå¯é ï¼Œä½†ä½œä¸º fallback)
            submitter_name = getattr(task, 'from_user_name', '') or user_map.get(user_id, "æœªçŸ¥ç”¨æˆ·")
            
            commit_time = getattr(task, 'commit_time', now)
            dt = datetime.fromtimestamp(int(commit_time))
            date_str = dt.strftime('%Y-%m-%d')
            
            # ä¼˜å…ˆä½¿ç”¨ user_id ç»„åˆé”®
            if user_id:
                key = (user_id, date_str)
            else:
                key = (submitter_name, date_str)
            
            # æ¯”è¾ƒ commit_timeï¼Œä¿ç•™æœ€æ–°çš„
            if key in filtered_tasks_map:
                existing_task = filtered_tasks_map[key]
                existing_time = getattr(existing_task, 'commit_time', 0)
                if int(commit_time) > int(existing_time):
                    filtered_tasks_map[key] = task
            else:
                filtered_tasks_map[key] = task
                
        final_tasks = list(filtered_tasks_map.values())
        print(f"ğŸ§¹ è¿‡æ»¤é‡å¤æ±‡æŠ¥åï¼Œå‰©ä½™ {len(final_tasks)} æ¡å¾…å¤„ç†ä»»åŠ¡ (ç­–ç•¥: æ¯å¤©æ¯äººä¿ç•™æœ€æ–°)ã€‚")

        for task in final_tasks:
            try:
                # 1. æå–åŸºç¡€ä¿¡æ¯
                user_id = getattr(task, 'from_user_id', '')
                submitter_name = getattr(task, 'from_user_name', '') or user_map.get(user_id, "æœªçŸ¥ç”¨æˆ·")
                rule_name = getattr(task, 'rule_name', 'æœªçŸ¥æ±‡æŠ¥')
                commit_time = getattr(task, 'commit_time', now)
                
                # è®¡ç®— Date Key
                dt = datetime.fromtimestamp(int(commit_time))
                date_str = dt.strftime('%Y-%m-%d')
                
                # 2. Transform (è½¬æ¢) - è§£ææ±‡æŠ¥å†…å®¹
                content_text = self._parse_form_data(task)
                if not content_text:
                    print(f"âš ï¸ è·³è¿‡ç©ºæ±‡æŠ¥: {submitter_name}")
                    continue

                # 3. Transform (è½¬æ¢) - ç¡®å®šæŠ¥å‘Šç±»å‹
                report_type = "å‘¨æŠ¥" if "å‘¨" in rule_name else "æ—¥æŠ¥"
                
                # 4. Transform (è½¬æ¢) - AI è¯Šæ–­
                print(f"ğŸ¤– æ­£åœ¨ AI è¯Šæ–­ {submitter_name} çš„{report_type} ({date_str})...")
                ai_result = await self._call_ai_diagnosis(report_type, content_text)
                
                # 5. Delete Old Records (åˆ é™¤æ—§è®°å½•)
                records_to_delete = []
                
                # Check by User ID
                if user_id:
                    key_id = (user_id, date_str)
                    if key_id in existing_map_by_id:
                        records_to_delete.extend(existing_map_by_id[key_id])
                
                # Check by Name (Fallback or Additional)
                key_name = (submitter_name, date_str)
                if key_name in existing_map_by_name:
                    # é¿å…é‡å¤æ·»åŠ  (å¦‚æœ ID å’Œ Name æŸ¥åˆ°äº†åŒä¸€æ¡)
                    for rid in existing_map_by_name[key_name]:
                        if rid not in records_to_delete:
                            records_to_delete.append(rid)
                            
                if records_to_delete:
                    print(f"ğŸ—‘ï¸ å‘ç° {submitter_name} åœ¨ {date_str} æœ‰ {len(records_to_delete)} æ¡æ—§è®°å½•ï¼Œæ­£åœ¨åˆ é™¤...")
                    for rid in records_to_delete:
                        await FeishuService.delete_bitable_record(
                            settings.FEISHU_BITABLE_APP_TOKEN,
                            settings.FEISHU_BITABLE_TABLE_ID,
                            rid
                        )
                
                # 6. Load (åŠ è½½) - å†™å…¥æ–°è®°å½•
                # å¤šç»´è¡¨æ ¼æ—¥æœŸå­—æ®µéœ€è¦æ¯«ç§’çº§æ—¶é—´æˆ³
                date_val = int(commit_time) * 1000
                
                bitable_fields = {
                    "æäº¤äºº": submitter_name,
                    "æ±‡æŠ¥äºº": [{"id": user_id}] if user_id else [],
                    "æ±‡æŠ¥æ—¥æœŸ": date_val,
                    "æŠ¥å‘Šç±»å‹": report_type,
                    "æ±‡æŠ¥å†…å®¹": content_text,
                    "AIè¯Šæ–­å»ºè®®": ai_result.get("advice", "æ— å»ºè®®"),
                    "è¯„åˆ†": str(ai_result.get("score", 0)),
                    "çŠ¶æ€": "å·²è¯Šæ–­"
                }
                
                success = await FeishuService.create_bitable_record(
                    settings.FEISHU_BITABLE_APP_TOKEN,
                    settings.FEISHU_BITABLE_TABLE_ID,
                    bitable_fields
                )
                
                if success:
                    print(f"âœ… {submitter_name} çš„æ•°æ®å·²å†™å…¥å¹¶è¯Šæ–­å®Œæˆã€‚")
                else:
                    print(f"âŒ {submitter_name} å†™å…¥å¤±è´¥ã€‚")
                # ---------------------------------
                    
            except Exception as e:
                logger.error(f"Error processing task: {e}", exc_info=True)
                print(f"âŒ å¤„ç†å‡ºé”™: {e}")

    def _parse_form_data(self, task) -> str:
        """
        è§£æ Task å¯¹è±¡ä¸­çš„ form_data
        """
        full_text = []
        # æ³¨æ„ï¼šlark_oapi è¿”å›çš„ task å¯¹è±¡ç»“æ„å¯èƒ½åŒ…å« form_content æˆ– form_data
        # è¿™é‡Œå‡è®¾ SDK è¿”å›çš„æ˜¯å¯¹è±¡ï¼Œæˆ‘ä»¬éœ€è¦éå†å®ƒçš„å­—æ®µ
        
        # å¦‚æœæ˜¯ SDK å¯¹è±¡ï¼Œé€šå¸¸ form_data æ˜¯ä¸€ä¸ª list
        # ä¿®æ­£: SDK è¿”å›çš„å­—æ®µåå¯èƒ½æ˜¯ form_contents
        form_contents = getattr(task, 'form_contents', [])
        # å…¼å®¹æ—§é€»è¾‘
        if not form_contents:
            form_contents = getattr(task, 'form_data', [])

        if not form_contents:
             return ""

        for field in form_contents:
            # å…¼å®¹ä¸åŒ SDK ç‰ˆæœ¬çš„å­—æ®µå
            name = getattr(field, 'field_name', getattr(field, 'name', ''))
            value = getattr(field, 'field_value', getattr(field, 'value', ''))
            
            # å…¼å®¹ value å¯èƒ½ä¸º None çš„æƒ…å†µ
            if value is None:
                # å°è¯• type ä¸º text çš„æƒ…å†µ
                if getattr(field, 'type', '') == 'text':
                     value = getattr(field, 'text_value', '')

            if value:
                full_text.append(f"ã€{name}ã€‘: {value}")
        
        return "\n".join(full_text)

    async def _call_ai_diagnosis(self, report_type: str, content: str) -> dict:
        """
        è°ƒç”¨ LLM è¿›è¡Œè¯Šæ–­
        """
        prompt = PROMPTS[PromptTemplate.REPORT_DIAGNOSIS].format(
            report_type=report_type,
            content=content
        )
        
        try:
            # ä½¿ç”¨ JSON æ¨¡å¼ (å¦‚æœ LLMClient æ”¯æŒï¼Œå¦åˆ™è§£ææ–‡æœ¬)
            # è¿™é‡Œå‡è®¾ LLMClient è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬å°è¯•è§£æ JSON
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_client.chat(messages)
            
            # æ¸…ç† Markdown ä»£ç å— (```json ... ```)
            if response.startswith("```"):
                lines = response.split("\n")
                if lines[0].strip().startswith("```"):
                    lines = lines[1:]
                if lines[-1].strip().startswith("```"):
                    lines = lines[:-1]
                response = "\n".join(lines)
                
            return json.loads(response)
            
        except Exception as e:
            logger.error(f"AI diagnosis failed: {e}")
            return {"advice": "AI è¯Šæ–­å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚", "score": 0}

    async def _prepare_weekly_data(self, start_time: int, end_time: int) -> dict:
        """
        æ‹‰å–å¹¶æ•´ç†ä¸€å‘¨çš„æ—¥æŠ¥æ•°æ®ï¼ŒæŒ‰ç”¨æˆ·åˆ†ç»„å¹¶æŒ‰æ—¥æœŸæ’åº
        :param start_time: å¼€å§‹æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        :param end_time: ç»“æŸæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        :return: {user_name: {'user_id': str, 'reports': [(date_str, content_text), ...]}, ...}
        """
        tasks = await FeishuService.get_report_tasks(start_time, end_time)
        
        if not tasks:
            return {}

        # æ‰¹é‡è·å–ç”¨æˆ·ä¿¡æ¯
        user_ids = list(set([task.from_user_id for task in tasks if hasattr(task, 'from_user_id')]))
        user_map = {}
        if user_ids:
            users = await FeishuService.batch_get_users(user_ids)
            if users:
                for user in users:
                    user_map[user.user_id] = user.name

        # æŒ‰ (user_id, date) åˆ†ç»„ï¼Œæ¯äººæ¯å¤©åªä¿ç•™æœ€æ–°ä¸€æ¡
        filtered_map = {}  # (user_id, date_str) -> (commit_time, task, user_name, user_id)
        
        for task in tasks:
            user_id = getattr(task, 'from_user_id', '')
            user_name = getattr(task, 'from_user_name', '') or user_map.get(user_id, "æœªçŸ¥ç”¨æˆ·")
            commit_time = getattr(task, 'commit_time', 0)
            dt = datetime.fromtimestamp(int(commit_time))
            date_str = dt.strftime('%Y-%m-%d')
            
            key = (user_id or user_name, date_str)
            
            if key in filtered_map:
                if int(commit_time) > filtered_map[key][0]:
                    filtered_map[key] = (int(commit_time), task, user_name, user_id)
            else:
                filtered_map[key] = (int(commit_time), task, user_name, user_id)

        # æŒ‰ç”¨æˆ·åˆ†ç»„ï¼ŒæŒ‰æ—¥æœŸæ’åº
        user_reports = {}  # user_name -> {'user_id': str, 'reports': [(date_str, content_text), ...]}
        
        for (uid_or_name, date_str), (_, task, user_name, user_id) in filtered_map.items():
            content_text = self._parse_form_data(task)
            if not content_text:
                continue
            
            if user_name not in user_reports:
                user_reports[user_name] = {'user_id': user_id, 'reports': []}
            user_reports[user_name]['reports'].append((date_str, content_text))

        # æŒ‰æ—¥æœŸæ’åº
        for user_name in user_reports:
            user_reports[user_name]['reports'].sort(key=lambda x: x[0])

        return user_reports

    def _format_weekly_reports(self, reports: list[tuple[str, str]]) -> str:
        """
        å°†ä¸€å‘¨çš„æ—¥æŠ¥åˆ—è¡¨æ ¼å¼åŒ–ä¸º LLM è¾“å…¥æ–‡æœ¬
        """
        parts = []
        for i, (date_str, content) in enumerate(reports, 1):
            parts.append(f"--- ç¬¬{i}å¤©: {date_str} ---")
            parts.append(content)
            parts.append("")
        return "\n".join(parts)

    @staticmethod
    def _extract_summary_and_score(full_content: str) -> tuple:
        """
        ä» LLM ç”Ÿæˆçš„ Markdown åˆ†ææŠ¥å‘Šä¸­æå–æ‘˜è¦å’Œè¯„åˆ†
        :param full_content: å®Œæ•´çš„ Markdown åˆ†ææ–‡æœ¬
        :return: (summary_text, score)
        """
        import re
        
        # æå–è¯„åˆ†ï¼šåŒ¹é… "å‘¨åº¦è¯„åˆ†: XX/100" æˆ– "è¯„åˆ†: XX/100"
        score = 0
        score_match = re.search(r'è¯„åˆ†[ï¼š:]\s*(\d+)\s*/\s*100', full_content)
        if score_match:
            score = int(score_match.group(1))
        
        # æå–æ‘˜è¦ï¼šåŒ¹é… "## æ‘˜è¦" åé¢çš„å†…å®¹
        summary = ""
        summary_match = re.search(r'##\s*æ‘˜è¦\s*\n([\s\S]*?)(?:\n##|\n#|\Z)', full_content)
        if summary_match:
            summary = summary_match.group(1).strip()
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°æ‘˜è¦ï¼Œç”¨å‰200å­—ä½œä¸ºæ‘˜è¦
        if not summary:
            # å»æ‰æ ‡é¢˜è¡Œï¼Œå–å‰200å­—
            lines = [l for l in full_content.split('\n') if not l.startswith('#')]
            summary = '\n'.join(lines)[:200].strip()
            if len(full_content) > 200:
                summary += "..."
        
        return summary, score

    async def _save_summary_to_bitable(self, user_name: str, user_id: str, date_range: str, 
                                       full_content: str, summary: str, score: int, report_type: str = "å‘¨æ€»ç»“"):
        """
        å°†å‘¨/æ—¥/æœˆæ€»ç»“ç»“æœå†™å…¥ Bitable
        :param report_type: "å‘¨æ€»ç»“", "æ—¥æ€»ç»“", æˆ– "æœˆæ€»ç»“"
        """
        if not settings.FEISHU_BITABLE_APP_TOKEN or not settings.FEISHU_BITABLE_TABLE_ID:
            logger.warning("Missing Bitable configuration, skipping write.")
            return False

        # å½“å‰æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰ä½œä¸ºæ±‡æŠ¥æ—¥æœŸ
        date_val = int(time.time()) * 1000
        
        bitable_fields = {
            "æäº¤äºº": user_name,
            "æ±‡æŠ¥äºº": [{"id": user_id}] if user_id else [],
            "æ±‡æŠ¥æ—¥æœŸ": date_val,
            "æŠ¥å‘Šç±»å‹": report_type,
            "æ±‡æŠ¥å†…å®¹": full_content,
            "AIè¯Šæ–­å»ºè®®": summary,
            "è¯„åˆ†": str(score),
            "çŠ¶æ€": "å·²æ€»ç»“",
            "åˆ†æå‘¨æœŸ": date_range
        }
        
        try:
            success = await FeishuService.create_bitable_record(
                settings.FEISHU_BITABLE_APP_TOKEN,
                settings.FEISHU_BITABLE_TABLE_ID,
                bitable_fields
            )
            if success:
                logger.info(f"âœ… {user_name} çš„{report_type}å·²å†™å…¥ Bitable")
            else:
                logger.error(f"âŒ {user_name} çš„{report_type}å†™å…¥ Bitable å¤±è´¥")
            return success
        except Exception as e:
            logger.error(f"Error saving {report_type} to Bitable: {e}", exc_info=True)
            return False

    async def weekly_recursive_summary_stream(self, start_time: int, end_time: int, 
                                                target_user_name: str = None,
                                                save_to_bitable: bool = True):
        """
        æµå¼ç”Ÿæˆä¸€å‘¨é€’å½’å¼è¿›æ­¥æ€»ç»“
        :param start_time: å¼€å§‹æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        :param end_time: ç»“æŸæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        :param target_user_name: æŒ‡å®šç”¨æˆ·åï¼ˆå¯é€‰ï¼‰
        :param save_to_bitable: æ˜¯å¦åœ¨å®Œæˆåå†™å…¥ Bitable
        :yields: æµå¼æ–‡æœ¬å—ï¼ˆæœ€åä¸€ä¸ª yield é¢å¤–é™„å¸¦å®Œæ•´å†…å®¹ï¼‰
        """
        user_reports = await self._prepare_weekly_data(start_time, end_time)
        
        if not user_reports:
            yield "âš ï¸ è¯¥æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°æ—¥æŠ¥æ•°æ®ã€‚"
            return

        if target_user_name:
            matched = {k: v for k, v in user_reports.items() if target_user_name in k}
            if not matched:
                yield f"âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ· {target_user_name} çš„æ—¥æŠ¥æ•°æ®ã€‚"
                return
            user_reports = matched

        user_list = list(user_reports.items())
        for idx, (user_name, user_data) in enumerate(user_list):
            if idx > 0:
                yield "\n\n---\n\n"

            user_id = user_data['user_id']
            reports = user_data['reports']
            date_range = f"{reports[0][0]} è‡³ {reports[-1][0]}" if len(reports) > 1 else reports[0][0]
            daily_reports_text = self._format_weekly_reports(reports)
            
            prompt = PROMPTS[PromptTemplate.WEEKLY_RECURSIVE_SUMMARY].format(
                user_name=user_name,
                date_range=date_range,
                daily_reports=daily_reports_text
            )
            
            messages = [{"role": "user", "content": prompt}]
            user_full_content = ""
            
            try:
                async for chunk in self.llm_client.chat_stream(
                    messages=messages,
                    temperature=0.7,
                    max_tokens=4000
                ):
                    user_full_content += chunk
                    yield chunk
                
                # æµå¼ç»“æŸåå†™å…¥ Bitable
                if save_to_bitable and user_full_content:
                    summary, score = self._extract_summary_and_score(user_full_content)
                    # åˆ¤æ–­æ˜¯å‘¨æ€»ç»“è¿˜æ˜¯æ—¥æ€»ç»“ï¼ˆæ ¹æ®æ—¥æœŸèŒƒå›´ï¼‰
                    is_single_day = len(reports) == 1
                    report_type = "æ—¥æ€»ç»“" if is_single_day else "å‘¨æ€»ç»“"
                    await self._save_summary_to_bitable(
                        user_name, user_id, date_range, user_full_content, summary, score, report_type
                    )
            except Exception as e:
                logger.error(f"Weekly summary stream failed for {user_name}: {e}")
                yield f"\n\nâŒ ç”Ÿæˆ {user_name} çš„å‘¨æ€»ç»“æ—¶å‡ºé”™: {str(e)}"

    async def weekly_summary_and_save(self, start_time: int, end_time: int):
        """
        éæµå¼ç”Ÿæˆå‘¨æ€»ç»“å¹¶å†™å…¥ Bitableï¼ˆä¾›å®šæ—¶ä»»åŠ¡è°ƒç”¨ï¼‰
        :param start_time: å¼€å§‹æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        :param end_time: ç»“æŸæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        :return: å¤„ç†çš„ç”¨æˆ·æ•°é‡
        """
        user_reports = await self._prepare_weekly_data(start_time, end_time)
        
        if not user_reports:
            logger.info("No report data found for weekly summary.")
            return 0

        processed_count = 0
        for user_name, user_data in user_reports.items():
            user_id = user_data['user_id']
            reports = user_data['reports']
            date_range = f"{reports[0][0]} è‡³ {reports[-1][0]}" if len(reports) > 1 else reports[0][0]
            daily_reports_text = self._format_weekly_reports(reports)
            
            prompt = PROMPTS[PromptTemplate.WEEKLY_RECURSIVE_SUMMARY].format(
                user_name=user_name,
                date_range=date_range,
                daily_reports=daily_reports_text
            )
            
            messages = [{"role": "user", "content": prompt}]
            
            try:
                response = await self.llm_client.chat(
                    messages=messages,
                    temperature=0.7,
                    max_tokens=4000
                )
                
                if response:
                    summary, score = self._extract_summary_and_score(response)
                    # åˆ¤æ–­æ˜¯å‘¨æ€»ç»“è¿˜æ˜¯æ—¥æ€»ç»“
                    is_single_day = len(reports) == 1
                    report_type = "æ—¥æ€»ç»“" if is_single_day else "å‘¨æ€»ç»“"
                    success = await self._save_summary_to_bitable(
                        user_name, user_id, date_range, response, summary, score, report_type
                    )
                    if success:
                        processed_count += 1
                        logger.info(f"Weekly summary for {user_name}: score={score}")
                        
            except Exception as e:
                logger.error(f"Weekly summary failed for {user_name}: {e}", exc_info=True)

        return processed_count

    # ===================== æ„å›¾è¯†åˆ« =====================

    async def recognize_summary_intent(self, user_input: str) -> dict:
        """
        ä½¿ç”¨ LLM è¯†åˆ«ç”¨æˆ·è¾“å…¥ä¸­çš„æ€»ç»“æ„å›¾
        :return: {"type": "daily|weekly|monthly|none", "date_info": "..."}
        """
        current_date = datetime.now().strftime('%Y-%m-%d')
        prompt = PROMPTS[PromptTemplate.SUMMARY_INTENT_RECOGNITION].format(
            user_input=user_input,
            current_date=current_date
        )
        
        messages = [{"role": "user", "content": prompt}]
        
        try:
            response = await self.llm_client.chat(
                messages=messages,
                temperature=0.1,
                max_tokens=200
            )
            
            if response:
                # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—åŒ…è£¹å’Œå…¶ä»–æ ¼å¼
                cleaned = response.strip()
                
                # ç§»é™¤ä»£ç å—æ ‡è®° (```json æˆ– ```)
                if cleaned.startswith("```"):
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦åçš„å†…å®¹
                    lines = cleaned.split("\n")
                    if len(lines) > 1:
                        cleaned = "\n".join(lines[1:])
                    if cleaned.endswith("```"):
                        cleaned = cleaned[:-3]
                    cleaned = cleaned.strip()
                
                # å°è¯•ä»æ–‡æœ¬ä¸­æå–JSONå¯¹è±¡
                import re
                json_match = re.search(r'\{[^}]+\}', cleaned)
                if json_match:
                    cleaned = json_match.group(0)
                
                result = json.loads(cleaned)
                if isinstance(result, dict) and "type" in result:
                    logger.info(f"Intent recognized: {result}")
                    return result
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"Intent recognition failed: {e}, response: {response[:200] if response else 'None'}, input: {user_input}")
        
        return {"type": "none", "date_info": ""}

    @staticmethod
    def parse_date_range(intent_type: str, date_info: str) -> tuple:
        """
        æ ¹æ®æ„å›¾è¯†åˆ«ç»“æœè®¡ç®—æ—¶é—´èŒƒå›´
        :return: (start_ts, end_ts, date_range_desc)
        """
        now = datetime.now()
        
        if intent_type == "daily":
            # è§£æå…·ä½“æ—¥æœŸ
            target_date = now
            
            if "æ˜¨å¤©" in date_info or "æ˜¨æ—¥" in date_info:
                target_date = now - timedelta(days=1)
            elif "å‰å¤©" in date_info:
                target_date = now - timedelta(days=2)
            elif "ä»Šå¤©" in date_info or "ä»Šæ—¥" in date_info or not date_info:
                target_date = now
            else:
                # å°è¯•è§£æ MM-DD æˆ– MæœˆDæ—¥ æ ¼å¼
                import re
                m = re.search(r'(\d{1,2})[æœˆ\-/](\d{1,2})', date_info)
                if m:
                    month = int(m.group(1))
                    day = int(m.group(2))
                    year = now.year
                    try:
                        target_date = datetime(year, month, day)
                    except ValueError:
                        pass
            
            # å½“å¤© 00:00:00 åˆ° 23:59:59
            day_start = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
            day_end = day_start.replace(hour=23, minute=59, second=59)
            date_desc = day_start.strftime('%Y-%m-%d')
            return int(day_start.timestamp()), int(day_end.timestamp()), date_desc

        elif intent_type == "weekly":
            days_since_monday = now.weekday()
            
            if "ä¸Šå‘¨" in date_info:
                monday = now.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=days_since_monday + 7)
                sunday = monday + timedelta(days=6, hours=23, minutes=59, seconds=59)
                date_desc = f"{monday.strftime('%Y-%m-%d')} è‡³ {sunday.strftime('%Y-%m-%d')} (ä¸Šå‘¨)"
                return int(monday.timestamp()), int(sunday.timestamp()), date_desc
            else:
                monday = now.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=days_since_monday)
                date_desc = f"{monday.strftime('%Y-%m-%d')} è‡³ {now.strftime('%Y-%m-%d')} (æœ¬å‘¨)"
                return int(monday.timestamp()), int(now.timestamp()), date_desc

        elif intent_type == "monthly":
            import re
            import calendar
            
            if "ä¸Šæœˆ" in date_info or "ä¸Šä¸ªæœˆ" in date_info:
                if now.month == 1:
                    target_year, target_month = now.year - 1, 12
                else:
                    target_year, target_month = now.year, now.month - 1
            else:
                # å°è¯•è§£æ "Xæœˆ"
                m = re.search(r'(\d{1,2})\s*æœˆ', date_info)
                if m:
                    target_month = int(m.group(1))
                    target_year = now.year
                    # å¦‚æœæŒ‡å®šçš„æœˆä»½å¤§äºå½“å‰æœˆï¼Œå¯èƒ½æŒ‡å»å¹´
                    if target_month > now.month:
                        target_year -= 1
                else:
                    # é»˜è®¤æœ¬æœˆ
                    target_year, target_month = now.year, now.month
            
            _, last_day = calendar.monthrange(target_year, target_month)
            month_start = datetime(target_year, target_month, 1, 0, 0, 0)
            month_end = datetime(target_year, target_month, last_day, 23, 59, 59)
            date_desc = f"{month_start.strftime('%Y-%m-%d')} è‡³ {month_end.strftime('%Y-%m-%d')} ({target_month}æœˆ)"
            return int(month_start.timestamp()), int(month_end.timestamp()), date_desc
        
        # fallback
        return 0, 0, ""

    # ===================== æ—¥æ€»ç»“ =====================

    async def daily_summary_stream(self, start_time: int, end_time: int,
                                     save_to_bitable: bool = True):
        """
        æµå¼ç”Ÿæˆæ—¥æ€»ç»“ï¼ˆå•å¤©å·¥ä½œè¯„ä¼°ï¼‰
        """
        user_reports = await self._prepare_weekly_data(start_time, end_time)
        
        if not user_reports:
            yield "âš ï¸ è¯¥æ—¥æœŸæ²¡æœ‰æ‰¾åˆ°æ—¥æŠ¥æ•°æ®ã€‚"
            return

        date_str = datetime.fromtimestamp(start_time).strftime('%Y-%m-%d')
        
        for user_name, user_data in user_reports.items():
            user_id = user_data['user_id']
            reports = user_data['reports']
            
            # æ—¥æ€»ç»“åªå–å½“å¤©çš„æ•°æ®
            daily_content = "\n".join([content for _, content in reports])
            
            prompt = PROMPTS[PromptTemplate.DAILY_SUMMARY].format(
                user_name=user_name,
                date_str=date_str,
                daily_content=daily_content
            )
            
            messages = [{"role": "user", "content": prompt}]
            user_full_content = ""
            
            try:
                async for chunk in self.llm_client.chat_stream(
                    messages=messages,
                    temperature=0.7,
                    max_tokens=3000
                ):
                    user_full_content += chunk
                    yield chunk
                
                if save_to_bitable and user_full_content:
                    summary, score = self._extract_summary_and_score(user_full_content)
                    await self._save_summary_to_bitable(
                        user_name, user_id, date_str, user_full_content, 
                        summary, score, "æ—¥æ€»ç»“"
                    )
            except Exception as e:
                logger.error(f"Daily summary stream failed for {user_name}: {e}")
                yield f"\n\nâŒ ç”Ÿæˆ {user_name} çš„æ—¥æ€»ç»“æ—¶å‡ºé”™: {str(e)}"

    # ===================== æœˆæ€»ç»“ =====================

    async def _compress_daily_for_monthly(self, user_reports: dict) -> dict:
        """
        å°†æ¯æ—¥æ—¥æŠ¥å‹ç¼©ä¸º â‰¤100 å­—çš„æ‘˜è¦ï¼ˆç”¨äºæœˆæ€»ç»“ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼‰
        :param user_reports: {user_name: {'user_id': str, 'reports': [(date_str, content), ...]}}
        :return: {user_name: {'user_id': str, 'summaries': [(date_str, summary_text), ...]}}
        """
        compressed = {}
        
        for user_name, user_data in user_reports.items():
            user_id = user_data['user_id']
            summaries = []
            
            for date_str, content in user_data['reports']:
                # å°è¯•ç”¨ç®€çŸ­ prompt å‹ç¼©
                try:
                    compress_prompt = f"è¯·ç”¨ä¸è¶…è¿‡100ä¸ªä¸­æ–‡å­—ç¬¦æ¦‚æ‹¬ä»¥ä¸‹å·¥ä½œæ—¥æŠ¥çš„æ ¸å¿ƒå†…å®¹,åªè¾“å‡ºæ¦‚æ‹¬æ–‡å­—,ä¸è¦ä»»ä½•å‰ç¼€:\n\n{content}"
                    messages = [{"role": "user", "content": compress_prompt}]
                    summary = await self.llm_client.chat(
                        messages=messages,
                        temperature=0.3,
                        max_tokens=200
                    )
                    if summary:
                        # ç¡®ä¿ä¸è¶…è¿‡100å­—
                        summary = summary.strip()[:100]
                    else:
                        summary = content[:100] + "..." if len(content) > 100 else content
                except Exception:
                    summary = content[:100] + "..." if len(content) > 100 else content
                
                summaries.append((date_str, summary))
            
            compressed[user_name] = {'user_id': user_id, 'summaries': summaries}
        
        return compressed

    async def monthly_summary_stream(self, start_time: int, end_time: int,
                                       save_to_bitable: bool = True):
        """
        æµå¼ç”Ÿæˆæœˆæ€»ç»“ï¼ˆåŸºäºå‹ç¼©çš„æ¯æ—¥æ‘˜è¦ï¼‰
        """
        user_reports = await self._prepare_weekly_data(start_time, end_time)
        
        if not user_reports:
            yield "âš ï¸ è¯¥æœˆä»½æ²¡æœ‰æ‰¾åˆ°æ—¥æŠ¥æ•°æ®ã€‚"
            return

        # Step 1: å‹ç¼©æ¯æ—¥æ—¥æŠ¥ä¸ºæ‘˜è¦
        yield "ğŸ“ æ­£åœ¨æ•´ç†æœˆåº¦æ•°æ®...\n\n"
        compressed_data = await self._compress_daily_for_monthly(user_reports)
        
        month_range = f"{datetime.fromtimestamp(start_time).strftime('%Y-%m-%d')} è‡³ {datetime.fromtimestamp(end_time).strftime('%Y-%m-%d')}"
        
        for user_name, user_data in compressed_data.items():
            user_id = user_data['user_id']
            summaries = user_data['summaries']
            
            # æ ¼å¼åŒ–æ¯æ—¥æ‘˜è¦
            daily_summaries_text = "\n".join([
                f"- {date_str}: {summary}" for date_str, summary in summaries
            ])
            
            prompt = PROMPTS[PromptTemplate.MONTHLY_SUMMARY].format(
                user_name=user_name,
                month_range=month_range,
                daily_summaries=daily_summaries_text
            )
            
            messages = [{"role": "user", "content": prompt}]
            user_full_content = ""
            
            try:
                async for chunk in self.llm_client.chat_stream(
                    messages=messages,
                    temperature=0.7,
                    max_tokens=4000
                ):
                    user_full_content += chunk
                    yield chunk

                # å†™å…¥ Bitable
                if save_to_bitable and user_full_content:
                    summary, score = self._extract_summary_and_score(user_full_content)
                    await self._save_summary_to_bitable(
                        user_name, user_id, month_range, user_full_content, 
                        summary, score, "æœˆæ€»ç»“"
                    )

            except Exception as e:
                logger.error(f"Monthly summary stream failed for {user_name}: {e}")
                yield f"\n\nâŒ ç”Ÿæˆ {user_name} çš„æœˆæ€»ç»“æ—¶å‡ºé”™: {str(e)}"


    # ===================== éæµå¼ï¼ˆå®šæ—¶ä»»åŠ¡ç”¨ï¼‰ =====================

    async def daily_summary_and_save(self, start_time: int, end_time: int):
        """éæµå¼æ—¥æ€»ç»“å¹¶å†™å…¥ Bitableï¼ˆå®šæ—¶ä»»åŠ¡ç”¨ï¼‰"""
        user_reports = await self._prepare_weekly_data(start_time, end_time)
        if not user_reports:
            logger.info("No report data found for daily summary.")
            return 0

        date_str = datetime.fromtimestamp(start_time).strftime('%Y-%m-%d')
        processed_count = 0
        
        for user_name, user_data in user_reports.items():
            user_id = user_data['user_id']
            reports = user_data['reports']
            daily_content = "\n".join([content for _, content in reports])
            
            prompt = PROMPTS[PromptTemplate.DAILY_SUMMARY].format(
                user_name=user_name,
                date_str=date_str,
                daily_content=daily_content
            )
            messages = [{"role": "user", "content": prompt}]
            
            try:
                response = await self.llm_client.chat(
                    messages=messages, temperature=0.7, max_tokens=3000
                )
                if response:
                    summary, score = self._extract_summary_and_score(response)
                    success = await self._save_summary_to_bitable(
                        user_name, user_id, date_str, response, summary, score, "æ—¥æ€»ç»“"
                    )
                    if success:
                        processed_count += 1
            except Exception as e:
                logger.error(f"Daily summary failed for {user_name}: {e}", exc_info=True)
        
        return processed_count

    async def monthly_summary_and_save(self, start_time: int, end_time: int):
        """éæµå¼æœˆæ€»ç»“å¹¶å†™å…¥ Bitableï¼ˆå®šæ—¶ä»»åŠ¡ç”¨ï¼‰"""
        user_reports = await self._prepare_weekly_data(start_time, end_time)
        if not user_reports:
            logger.info("No report data found for monthly summary.")
            return 0

        compressed_data = await self._compress_daily_for_monthly(user_reports)
        month_range = f"{datetime.fromtimestamp(start_time).strftime('%Y-%m-%d')} è‡³ {datetime.fromtimestamp(end_time).strftime('%Y-%m-%d')}"
        processed_count = 0
        
        for user_name, user_data in compressed_data.items():
            user_id = user_data['user_id']
            summaries = user_data['summaries']
            daily_summaries_text = "\n".join([
                f"- {date_str}: {summary}" for date_str, summary in summaries
            ])
            
            prompt = PROMPTS[PromptTemplate.MONTHLY_SUMMARY].format(
                user_name=user_name,
                month_range=month_range,
                daily_summaries=daily_summaries_text
            )
            messages = [{"role": "user", "content": prompt}]
            
            try:
                response = await self.llm_client.chat(
                    messages=messages, temperature=0.7, max_tokens=4000
                )
                if response:
                    summary, score = self._extract_summary_and_score(response)
                    success = await self._save_summary_to_bitable(
                        user_name, user_id, month_range, response, summary, score, "æœˆæ€»ç»“"
                    )
                    if success:
                        processed_count += 1
            except Exception as e:
                logger.error(f"Monthly summary failed for {user_name}: {e}", exc_info=True)
        
        return processed_count
